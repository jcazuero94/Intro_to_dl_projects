{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please\n",
    "refer to https://github.com/jcazuero94/Intro_to_dl_projects/tree/main/gan_monet to include images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs\n",
    "Juan Camilo Azuero\n",
    "## Introduction\n",
    "This notebook is part of a mini-project of the course Introduction to Deep Learning from the University of Colorado Boulder, and includes a participation in the <a href=\"https://www.kaggle.com/competitions/gan-getting-started/\">I'm something of a painter myself</a> competition from Kaggle. The goal of the competition is to develop a Generative Adversial Network that can transfer the style of Monet paintings into an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:17.782250Z",
     "iopub.status.busy": "2022-08-04T22:42:17.781793Z",
     "iopub.status.idle": "2022-08-04T22:42:24.577300Z",
     "shell.execute_reply": "2022-08-04T22:42:24.576148Z",
     "shell.execute_reply.started": "2022-08-04T22:42:17.782220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import PIL\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:24.582823Z",
     "iopub.status.busy": "2022-08-04T22:42:24.582139Z",
     "iopub.status.idle": "2022-08-04T22:42:25.160865Z",
     "shell.execute_reply": "2022-08-04T22:42:25.159871Z",
     "shell.execute_reply.started": "2022-08-04T22:42:24.582786Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE_RESIZE = [286, 286]\n",
    "IMAGE_SIZE = [256, 256, 3]\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition is based on a Dataset of 300 images of Monet paintings and around 7000 other images. These images come grouped in distinct folders and are available as jpgs and in the Tensorflow record format. Considering that the tfrec format is faster for training we are going to use the images with that format as input. The data fetching and creation of tensorflow datasets is based on the <a href=\"https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\">Monet CycleGAN Tutorial</a> recomended in the competitions page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:25.163343Z",
     "iopub.status.busy": "2022-08-04T22:42:25.162725Z",
     "iopub.status.idle": "2022-08-04T22:42:26.073093Z",
     "shell.execute_reply": "2022-08-04T22:42:26.072131Z",
     "shell.execute_reply.started": "2022-08-04T22:42:25.163304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get filenames\n",
    "MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n",
    "PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:26.077632Z",
     "iopub.status.busy": "2022-08-04T22:42:26.076632Z",
     "iopub.status.idle": "2022-08-04T22:42:26.085607Z",
     "shell.execute_reply": "2022-08-04T22:42:26.084512Z",
     "shell.execute_reply.started": "2022-08-04T22:42:26.077593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions used to read te tfrecord datasets\n",
    "def decode_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, IMAGE_SIZE)\n",
    "    return image\n",
    "def read_tfrecord(example):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example['image'])\n",
    "    return image\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:26.089393Z",
     "iopub.status.busy": "2022-08-04T22:42:26.089076Z",
     "iopub.status.idle": "2022-08-04T22:42:28.902551Z",
     "shell.execute_reply": "2022-08-04T22:42:28.901584Z",
     "shell.execute_reply.started": "2022-08-04T22:42:26.089353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading of datasets\n",
    "monet_ds = load_dataset(MONET_FILENAMES, labeled=True)\n",
    "photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training data augmentation will be implemented by including random flip and zoom layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:28.904783Z",
     "iopub.status.busy": "2022-08-04T22:42:28.904057Z",
     "iopub.status.idle": "2022-08-04T22:42:28.935283Z",
     "shell.execute_reply": "2022-08-04T22:42:28.934459Z",
     "shell.execute_reply.started": "2022-08-04T22:42:28.904739Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomZoom(-0.2,-0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is memory intensive, so after some testing a batch size of 5 was selected. The following function prepare datasets for training and testing by setting the batch parameter and including data augmentation and suffling if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:28.936957Z",
     "iopub.status.busy": "2022-08-04T22:42:28.936520Z",
     "iopub.status.idle": "2022-08-04T22:42:28.942939Z",
     "shell.execute_reply": "2022-08-04T22:42:28.941983Z",
     "shell.execute_reply.started": "2022-08-04T22:42:28.936920Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "def prepare(ds, shuffle=False, augment=False):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    # Batch all datasets.\n",
    "    ds = ds.batch(batch_size)\n",
    "    # Use data augmentation only on the training set.\n",
    "    if augment:\n",
    "        ds = ds.map(data_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    # Use buffered prefetching on all datasets.\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:28.945029Z",
     "iopub.status.busy": "2022-08-04T22:42:28.944712Z",
     "iopub.status.idle": "2022-08-04T22:42:29.203282Z",
     "shell.execute_reply": "2022-08-04T22:42:29.202033Z",
     "shell.execute_reply.started": "2022-08-04T22:42:28.944995Z"
    }
   },
   "outputs": [],
   "source": [
    "monet_ds = prepare(monet_ds, shuffle=True, augment=True)\n",
    "photo_ds_train = prepare(photo_ds, shuffle=True, augment=True)\n",
    "photo_ds_test = prepare(photo_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see some images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:42:29.205273Z",
     "iopub.status.busy": "2022-08-04T22:42:29.204823Z",
     "iopub.status.idle": "2022-08-04T22:42:32.213876Z",
     "shell.execute_reply": "2022-08-04T22:42:32.212954Z",
     "shell.execute_reply.started": "2022-08-04T22:42:29.205239Z"
    }
   },
   "outputs": [],
   "source": [
    "example_monet = next(iter(monet_ds))\n",
    "example_photo = next(iter(photo_ds_test))\n",
    "plt.subplot(121)\n",
    "plt.title('Photo')\n",
    "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
    "plt.subplot(122)\n",
    "plt.title('Monet')\n",
    "plt.imshow(example_monet[0] * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](../Img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model architechture a CycleGAN (<a href=\"https://arxiv.org/abs/1703.10593\">original paper</a>) was selected because it achieves state of the art perfonce in image translation tasks. The CycleGAN implementation in this notebook is based on the <a href=\"https://keras.io/examples/generative/cyclegan/\">code example of the keras library</a> which follows the 2017 paper. <br>\n",
    "The CycleGAN is formed by 2 different GANs:\n",
    "- GAN 1:\n",
    " - Generator model ($G$) that includes Monet's style into a regular pricture\n",
    " - Discriminator model ($D_Y$) that diferentiates between Monet paintings and the output of the generator\n",
    "- GAN 2: \n",
    " - Generator model ($F$) strips out Monet's style from a picture\n",
    " - Discriminator model ($D_X$) that diferentiates between Monet paintings and the output of the generator<br>\n",
    " \n",
    "In the followign diagram the $X$ represents the space of pictures without Monet style, and $Y$ the space with the style of Monets paintings. The left cell from the diagram shows how a picture enters the monet generator, and then the $F$ generator. This process generates 3 losses, two losses given from the $D_x$ and $D_y$ discrimators, and a third loss that measures the distance betwen the original picture $x$ and the twice transformed $G(F(x))$. An analogous process of feeding pictures of monet paintings into the pair of GANs result in another corresponding losses. The model is then optimized by using gradient descent over the mentioned losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../GanDiagram.png)\n",
    "<br>*Taken from the tensorflow tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of a CycleGAN includes RES-NET blocks in the generators and minimizes the mean squared error for the discriminators and the mean absolute error when comparing an image $x$ with $G(F(x))$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:52.693312Z",
     "iopub.status.busy": "2022-08-04T21:28:52.692995Z",
     "iopub.status.idle": "2022-08-04T21:28:52.699326Z",
     "shell.execute_reply": "2022-08-04T21:28:52.697303Z",
     "shell.execute_reply.started": "2022-08-04T21:28:52.693283Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:52.701849Z",
     "iopub.status.busy": "2022-08-04T21:28:52.701159Z",
     "iopub.status.idle": "2022-08-04T21:28:52.717906Z",
     "shell.execute_reply": "2022-08-04T21:28:52.716903Z",
     "shell.execute_reply.started": "2022-08-04T21:28:52.701811Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:52.721105Z",
     "iopub.status.busy": "2022-08-04T21:28:52.720139Z",
     "iopub.status.idle": "2022-08-04T21:28:52.735602Z",
     "shell.execute_reply": "2022-08-04T21:28:52.734465Z",
     "shell.execute_reply.started": "2022-08-04T21:28:52.721068Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=IMAGE_SIZE, name=name + \"_img_input\")\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(img_input, x, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:52.737627Z",
     "iopub.status.busy": "2022-08-04T21:28:52.737245Z",
     "iopub.status.idle": "2022-08-04T21:28:55.125062Z",
     "shell.execute_reply": "2022-08-04T21:28:55.124004Z",
     "shell.execute_reply.started": "2022-08-04T21:28:52.737590Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=IMAGE_SIZE, name=name + \"_img_input\")\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        (4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(num_downsampling):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(1, 1),\n",
    "            )\n",
    "    x = layers.Conv2D(\n",
    "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:55.127360Z",
     "iopub.status.busy": "2022-08-04T21:28:55.126941Z",
     "iopub.status.idle": "2022-08-04T21:28:55.150523Z",
     "shell.execute_reply": "2022-08-04T21:28:55.149330Z",
     "shell.execute_reply.started": "2022-08-04T21:28:55.127321Z"
    }
   },
   "outputs": [],
   "source": [
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Horse and y is zebra\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    we can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adverserial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:55.152840Z",
     "iopub.status.busy": "2022-08-04T21:28:55.152237Z",
     "iopub.status.idle": "2022-08-04T21:28:55.184219Z",
     "shell.execute_reply": "2022-08-04T21:28:55.183322Z",
     "shell.execute_reply.started": "2022-08-04T21:28:55.152803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback is used to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:55.186478Z",
     "iopub.status.busy": "2022-08-04T21:28:55.185538Z",
     "iopub.status.idle": "2022-08-04T21:28:55.196448Z",
     "shell.execute_reply": "2022-08-04T21:28:55.195400Z",
     "shell.execute_reply.started": "2022-08-04T21:28:55.186442Z"
    }
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
    "\n",
    "    def __init__(self, num_img=7):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "        for i, img in enumerate(photo_ds_test.take(self.num_img)):\n",
    "            prediction = self.model.gen_G(img)[0].numpy()\n",
    "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "            img_f = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "            if np.random.rand() < 1/self.num_img:\n",
    "                break\n",
    "        ax[0].imshow(img_f)\n",
    "        ax[1].imshow(prediction)\n",
    "        ax[0].set_title(\"Input image\")\n",
    "        ax[1].set_title(\"Translated image\")\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[1].axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:55.199072Z",
     "iopub.status.busy": "2022-08-04T21:28:55.197910Z",
     "iopub.status.idle": "2022-08-04T21:28:55.210093Z",
     "shell.execute_reply": "2022-08-04T21:28:55.208966Z",
     "shell.execute_reply.started": "2022-08-04T21:28:55.199036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "plotter = GANMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T21:28:55.214425Z",
     "iopub.status.busy": "2022-08-04T21:28:55.212531Z",
     "iopub.status.idle": "2022-08-04T22:09:11.149143Z",
     "shell.execute_reply": "2022-08-04T22:09:11.148207Z",
     "shell.execute_reply.started": "2022-08-04T21:28:55.214384Z"
    }
   },
   "outputs": [],
   "source": [
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((photo_ds_train, monet_ds)),\n",
    "    epochs=50,\n",
    "    callbacks=[plotter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image2](../Img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the pictures show, the images seem to have a little of Monet's style, but still has a lot to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell of code is base on the CycleGAN tutorial used also for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:14:00.842100Z",
     "iopub.status.busy": "2022-08-04T22:14:00.841258Z",
     "iopub.status.idle": "2022-08-04T22:14:00.848840Z",
     "shell.execute_reply": "2022-08-04T22:14:00.845887Z",
     "shell.execute_reply.started": "2022-08-04T22:14:00.842064Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../images/')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:17:44.535117Z",
     "iopub.status.busy": "2022-08-04T22:17:44.534482Z",
     "iopub.status.idle": "2022-08-04T22:25:02.024887Z",
     "shell.execute_reply": "2022-08-04T22:25:02.023860Z",
     "shell.execute_reply.started": "2022-08-04T22:17:44.535081Z"
    }
   },
   "outputs": [],
   "source": [
    "cont = 1\n",
    "for imgs in photo_ds_test:\n",
    "    for img in imgs:\n",
    "        prediction = cycle_gan_model.gen_G(tf.reshape(img,(1,256,256,3)), training=False)[0].numpy()\n",
    "        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "        im = PIL.Image.fromarray(prediction)\n",
    "        im.save(f\"../images/{cont}.jpg\")\n",
    "        cont += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T22:25:02.038171Z",
     "iopub.status.busy": "2022-08-04T22:25:02.037440Z",
     "iopub.status.idle": "2022-08-04T22:25:05.407034Z",
     "shell.execute_reply": "2022-08-04T22:25:05.406061Z",
     "shell.execute_reply.started": "2022-08-04T22:25:02.038134Z"
    }
   },
   "outputs": [],
   "source": [
    "shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I learned how to load an image dataset from tfrecords which are considerably faster, what is a CycleGAN, and how to implement one. The model constructed achieved style translation between images, but can still improve a lot. One factor that could improve a lot training speed is to use TPUs. I tried to use the TPU available through Kaggle following the <a href=\"https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\">Monet CycleGAN Tutorial</a>, but was not able to get it to work in the time required by the project. Another factor that could improve model performance is complementing the data augmentation pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
